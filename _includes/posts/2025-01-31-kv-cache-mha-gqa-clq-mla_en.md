# Chasing Efficient KV-Cache: Three Research Directions We Have Taken on Model Architectures

1. Less Heads: MHA->MQA->GQA
2. Low-Rank: MHA->MLA
3. Cross Layer: MHA->CLA

## Reference

<div id="refer-gshard"></div> [0] Brandon, et al. **Reducing Transformer Key-Value Cache Size with
Cross-Layer Attention** *38th Conference on Neural Information Processing Systems.*, 2024.

